{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM/t8vNQvVkvd4n1FcZcfjB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/Predict-CVE-CWE/blob/main/CVE_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCfFC6KcomhI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/Meta/meta-learning-bert\""
      ],
      "metadata": {
        "id": "6NWzuYViotwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd\n",
        "%ls -al"
      ],
      "metadata": {
        "id": "ot49ketrrVRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/mailong25/meta-learning-bert.git"
      ],
      "metadata": {
        "id": "lEaxneSypFGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Set 확인 "
      ],
      "metadata": {
        "id": "-nCW6CbSx-HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## How to build an accurate sentiment analysis model with handful training examples\n",
        "\n",
        "# 취약점 설명을 유형에 따라 150가지 범주로 범주로 분류하는 모델을 교육하려고 한다.\n",
        "# 각 취약점 및 유형에 대해 단일 모델을 사용한다.\n",
        "# 일부 취약점에서는 제한된 수의 학습 예제만 가지고 있다. low-resource domain \n",
        "\n",
        "# Let inspect the data\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/Meta/SD_Vulnerability_Dataset.xlsx\")\n",
        "#data = data[['index','CVE-ID','DESCRIPTION', 'CWE-ID','CWE-NAME']]\n",
        "data = data[['DESCRIPTION', 'CWE-ID']]\n",
        "# 데이터 개수 확인  len : 21855\n",
        "#print(len(data))\n",
        "# 데이터 확인 \n",
        "data\n",
        "\n",
        "# 데이터 형태 확인\n",
        "# \"\"\"\n",
        "# index\t        : 1\n",
        "# CVE-ID\t        : CVE-1999-0199\n",
        "# DESCRIPTION\t    : manual/search.texi in the GNU C Library (aka g...\t\n",
        "# CWE-ID\t        : CWE-252\n",
        "# CWE-NAME        : Unchecked Return Value\n",
        "\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "Q5zHm_jJpHA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 해결 방안\n",
        "# 많은 학습 데이터를 가진 도메인을 활용하여 좋은 starting point를 만들고 이 시점부터 적은 데이터를 가진 특정 모델에 대한 학습한다. \n",
        "\n",
        "# 접근법\n",
        "# Meta learning : 적은 학습 데이터만으로 빠르게 학습해야하는 많은 상황을 시뮬레이션한다., 많은 상황을 반복할수록 적은 학습 데이터로 학습하는데 더 좋아지고 있다. \n",
        "# Support set : 적은 훈련 샘플\n",
        "# Query set :  학습 피드백을 제공. 모델은 이 피드백을 사용하여 학습 전략을 조정한다. \n"
      ],
      "metadata": {
        "id": "nz0uWUEdrQbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Split meta training and meta testing"
      ],
      "metadata": {
        "id": "dLjHU7ddBlkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CWE-ID 유일한 값별로 개수 세기 \n",
        "data_counts=data['CWE-ID'].value_counts()\n",
        "#print(data_counts)\n",
        "#print(i , \":\", data_counts.index[i]) # 유형 ID \n",
        "#print(i , \":\", data_counts.values[i]) # 유형 별 취약점 개수\n",
        "\n",
        "# 유형 ID\n",
        "id_of_vulnerability = data_counts.index.to_list()\n",
        "# 유형 별 취약점 개수 \n",
        "number_of_vulnerability = data_counts.values.tolist()\n",
        "\n",
        "\n",
        "# 취약점이 100개 이상인 유형 골라내기 \n",
        "high_resource_domains = []\n",
        "\n",
        "# 유형 만큼 반복 돌면서 train 데이터와 test 데이터 분리  \n",
        "for i in range(len(data_counts)):\n",
        "    # 취약점 개수가 100보다 큰 경우 train_example로 추가 , \n",
        "    # 취약점 개수가 100보다 작은 경우 test_example로 추가\n",
        "    #if number_of_vulnerability[i] > 100:\n",
        "    if number_of_vulnerability[i] > 100: \n",
        "        high_resource_domains.append(id_of_vulnerability[i])\n",
        "\n",
        "\n",
        "#print(len(high_resource_domains))\n",
        "#print(high_resource_domains)"
      ],
      "metadata": {
        "id": "K41DntFGrTiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 인코딩\n",
        "label_dict = {}\n",
        "for index, cwe_label in enumerate(id_of_vulnerability):\n",
        "    #print(\"index: \" ,index)\n",
        "    #print(cwe_label)\n",
        "    label_dict[cwe_label] = index\n",
        "#label_dict\n"
      ],
      "metadata": {
        "id": "Z85FFELXNwqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 데이터 개수에 따른 유형 분류\n",
        "train_examples = [] # 학습 데이터 개수가 많은 유형 100 < \n",
        "test_examples = [] # 학습 데이터 개수가 적은 유형  100 > \n",
        "#print(type(data))\n",
        "for index , row in data.iterrows():\n",
        "    #print(\"\\n\\nr : \", row)\n",
        "    if row['CWE-ID'] in high_resource_domains:\n",
        "        #print(\"true\")\n",
        "        train_examples.append(row)\n",
        "    else:\n",
        "        #print(\"false\")\n",
        "        test_examples.append(row)\n",
        "\n",
        "\n",
        "#print(len(train_examples), len(test_examples))"
      ],
      "metadata": {
        "id": "3rUHtG615j6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(train_examples[0])\n",
        "#print(\"\\n\",train_examples[0]['CWE-ID']) # list"
      ],
      "metadata": {
        "id": "OO8-c-PtGab-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "f8-TZPQm6iU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create  Mata learning Task (task.py)"
      ],
      "metadata": {
        "id": "YMWZCYIKx3Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import torch \n",
        "from torch.utils.data import Dataset \n",
        "import numpy as np \n",
        "import collections \n",
        "import random \n",
        "import json, pickle \n",
        "from torch.utils.data import TensorDataset # DataLoader : 배치를 쉽게 사용, TensorDataset : X, Y 를 텐서로 묶기 위해 사용\n",
        "\n",
        "#LABEL_MAP = {'positive' :0 , 'negative' :1, 0:'positive', 1:'negative'}\n",
        "\n",
        "class MetaTask(Dataset):\n",
        "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
        "        print(\"\\nMeta Task init start\")\n",
        "        \"\"\"\n",
        "        : param sample: list of samples\n",
        "        : param num-task: number of training tasks.\n",
        "        : param k_support : number of support sample per task\n",
        "        : param k_query : number of query sample per task \n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\" eamples[0]\n",
        "        index                                                          2\n",
        "        CVE-ID                                             CVE-1999-0284\n",
        "        DESCRIPTION    Denial of service to NT mail servers including...\n",
        "        CWE-ID                                                   CWE-120\n",
        "        CWE-NAME       Buffer Copy without Checking Size of Input ('C...\n",
        "        \"\"\"\n",
        "\n",
        "        self.examples = examples\n",
        "        random.shuffle(self.examples)\n",
        "\n",
        "        self.num_task = num_task\n",
        "        self.k_support = k_support\n",
        "        self.k_query = k_query\n",
        "        self.tokenizer = tokenizer \n",
        "        self.max_seq_length = 512 #128\n",
        "        self.create_batch(self.num_task)\n",
        "\n",
        "    def create_batch(self, num_task):\n",
        "        #print(\"Meta Task create_batch start\")\n",
        "        self.supports = [] # support set \n",
        "        self.queries = [] # query set\n",
        "\n",
        "        for b in range(num_task): # for each task \n",
        "        #for b in range(1): # for each task \n",
        "            # 1. select domain randomly\n",
        "            # 각 태스크에 들어갈 2개의 유형을 선택 \n",
        "            exam_train = []\n",
        "            exam_test = []\n",
        "            for i in range(0,3): # self.num_labels\n",
        "                tmp_selected_examples = []\n",
        "                \n",
        "\n",
        "                domain = random.choice(self.examples)['CWE-ID'] # 'CWE-ID': 'CWE-94'\n",
        "                domainExamples = [e for e in self.examples if e['CWE-ID'] == domain] # domain에 해당하는 모든 샘플 추출\n",
        "                #print(\"domain :\" , domain)\n",
        "                #print(\"len(domainExamples) : \" , len(domainExamples))\n",
        "                \n",
        "                # 1. select k_support + k_query examples from domain randomly \n",
        "                #  random.sample() : 지정한 숫자만큼의 요소들을 랜덤으로 뽑아 리스트로 반환\n",
        "                # 취약점 개수가 100개 이하면 support , query 의 개수를 더했을 때 갯수가 안맞아서 if 사용\n",
        "                if len(domainExamples) <= 50:\n",
        "                    selected_examples = random.sample(domainExamples, len(domainExamples)) # 전부 선택\n",
        "                    random.shuffle(selected_examples)\n",
        "                    for j in range (len(selected_examples)):\n",
        "                        tmp_selected_examples.append({ 'DESCRIPTION' :selected_examples[j]['DESCRIPTION'], 'CWE-ID':i})\n",
        "                    exam_train.extend(tmp_selected_examples[:1]) # 1\n",
        "                    exam_test.extend(tmp_selected_examples[1:]) # 나머지\n",
        "                    # selected_examples = random.sample(domainExamples, len(domainExamples)) \n",
        "                    # random.shuffle(selected_examples)\n",
        "                    # exam_train = selected_examples[:self.k_support]\n",
        "                    # exam_test = selected_examples[self.k_support:]   \n",
        "                elif len(domainExamples) <= 100:\n",
        "                    selected_examples = random.sample(domainExamples, len(domainExamples)) # 전부 선택\n",
        "                    random.shuffle(selected_examples)\n",
        "                    for j in range (len(selected_examples)):\n",
        "                        tmp_selected_examples.append({ 'DESCRIPTION' :selected_examples[j]['DESCRIPTION'], 'CWE-ID':i})\n",
        "                    \n",
        "                    train_num = int(len(domainExamples)/2)\n",
        "                    \n",
        "                    #print(\"train_num : \" , train_num)\n",
        "\n",
        "                    \n",
        "                    exam_train.extend(tmp_selected_examples[: train_num]) # 절반\n",
        "                    exam_test.extend(tmp_selected_examples[train_num:]) # 절반\n",
        "                    #exam_train = selected_examples[:self.k_support]\n",
        "                    #exam_test = selected_examples[self.k_support:]\n",
        "\n",
        "                else:\n",
        "                    #print(\"\\n\" ,i , \" : domainExamples : \", domainExamples)\n",
        "                    selected_examples = random.sample(domainExamples, self.k_support + self.k_query) # 65\n",
        "                    random.shuffle(selected_examples)\n",
        "                    #print(\"selected_examples : \", selected_examples[0]['CWE-ID'])\n",
        "                    for j in range (len(selected_examples)):\n",
        "                        tmp_selected_examples.append({ 'DESCRIPTION' :selected_examples[j]['DESCRIPTION'], 'CWE-ID':i})\n",
        "\n",
        "                    #print(tmp_selected_examples)\n",
        "                    #print(tmp_selected_examples)\n",
        "                    #print(np.shape(tmp_selected_examples))\n",
        "\n",
        "                    exam_train.extend(tmp_selected_examples[:self.k_support]) # [:50]\n",
        "                    exam_test.extend(tmp_selected_examples[self.k_support:]) # [50:]\n",
        "                    #exam_train = selected_examples[:self.k_support]\n",
        "                    #exam_test = selected_examples[self.k_support:]\n",
        "                #print(\"len exam_train : \" , len(exam_train))\n",
        "                #print(\"len exam_test : \" , len(exam_test))\n",
        "                del selected_examples\n",
        "                del tmp_selected_examples\n",
        "\n",
        "\n",
        "            self.supports.append(exam_train)\n",
        "            self.queries.append(exam_test)\n",
        "            del exam_train\n",
        "            del exam_test\n",
        "            #print(\"len() : \" , len(self.supports))\n",
        "            #print(\"supports : \", self.supports)\n",
        "            #print(\"supports shape : \", np.shape(self.supports))\n",
        "            #print(\"supports type : \", type(self.supports))\n",
        "\n",
        "\n",
        "            #self.supports = self.supports[0] + self.supports[1]\n",
        "            #print(\"&supports shape : \", np.shape(self.supports))\n",
        "            #print(\"&supports type : \", type(self.supports))\n",
        "\n",
        "            #print(\"queries : \" , self.queries[0])\n",
        "            #print(\"queries shape : \", np.shape(self.queries))\n",
        "            #print(\"queries type : \", type(self.queries))\n",
        "\n",
        "\n",
        "    def create_feature_set(self, examples):\n",
        "        #print(\"Meta Task  create_feature_set start\")\n",
        "        #print(\"examples shpae : \" ,np.shape(examples))\n",
        "        # torch.empty(num1,num2)는 (num1 x num2) 크기의 행렬을 구성하면서 초기화되지 않은 데이터로 값을 채움\n",
        "        all_input_ids       = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_attention_mask   = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_segment_ids     = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_label_ids       = torch.empty(len(examples), dtype = torch.long)\n",
        "        \n",
        "\n",
        "        # 변수 뒤에 _ 를 사용하는 이유는 파이썬 기본 시스템 변수명과 충돌을 피하기 위해서이다.\n",
        "        #print(\"Meta Task for examples tokenizer.encode start \")\n",
        "        for id_, example in enumerate(examples):\n",
        "            #print(\"example['DESCRIPTION'] : \" , example['DESCRIPTION'])\n",
        "            input_ids       = tokenizer.encode(example['DESCRIPTION'], truncation=True, max_length=512) # tokenizer.encode(example['text'])\n",
        "            #print(\"input_ids : \" , input_ids)\n",
        "            #print(\"input_ids len : \" , len(input_ids))\n",
        "            #print(\"input_ids shape : \" , np.shape(input_ids))\n",
        "            attention_mask  = [1] * len(input_ids)\n",
        "            segment_ids     = [0] * len(input_ids)\n",
        "\n",
        "            # 패딩\n",
        "            while len(input_ids) < self.max_seq_length: \n",
        "                input_ids.append(0)\n",
        "                attention_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "\n",
        "            # 영문으로 이루어진 label을 숫자로 표현\n",
        "            # 'label': 'positive' , LABEL_MAP = 'positive' :0 \n",
        "            #label_id = LABEL_MAP[example['label']]     \n",
        "            label_id = example['CWE-ID']\n",
        "            #print(\"example['CWE-ID'] : \", example['CWE-ID'])\n",
        "            #print(\"label_id : \" , label_id)\n",
        "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
        "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
        "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
        "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
        "\n",
        "        #print(\"Meta Task create tensor_set(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\")\n",
        "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
        "        return tensor_set\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #print(\"Meta Task getitem start\")\n",
        "        #print(\"Meta Task getitem index : \" , index)\n",
        "        #print(\"Meta Task getitem len supports : \" , len(self.supports))\n",
        "        #print(\"supports : \", self.supports)\n",
        "        #print(\"supports len : \", len(self.supports) )\n",
        "        support_set = self.create_feature_set(self.supports[index])\n",
        "        #print(\"support_set : \" , support_set)\n",
        "        #print(\"Meta Task getitem create_feature_set call _ suppoer set \")\n",
        "        query_set = self.create_feature_set(self.queries[index])\n",
        "        #print(\"Meta Task getitem create_feature_set call _ query set \")\n",
        "        return support_set, query_set \n",
        "\n",
        "    def __len__(self):\n",
        "        # 데이터 세트를 한 묶음으로 만들었기 때문에 집합의 작은 배치 크기를 샘플로 추출할 수 있습니다.\n",
        "        return self.num_task\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "y9YzWGb1x8rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch \n",
        "from transformers import BertModel, BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
        "#train = MetaTask(train_examples, num_task = 100, k_support=100, k_query=30, tokenizer = tokenizer)\n",
        "#train = MetaTask(train_examples, num_task = 100, k_support=50, k_query=15, tokenizer = tokenizer)\n",
        "#train = MetaTask(train_examples, num_task = 100, k_support=50, k_query=20, tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "31sZvsvt5-TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메타 태스크의 support set 처음 샘플 확인해보기\n",
        "#print(train.supports[0][0]['CWE-ID'])\n",
        "#print(\"\\n\")\n",
        "#print(train.queries[0][1:2])"
      ],
      "metadata": {
        "id": "yMPNhBnT6cFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 메타 태스크의 정보이다. support set과 query set을 포함하는 텐서 데이터세트\n",
        "#print(type(train))\n",
        "#train[0]\n",
        "#print(np.shape(train))\n",
        "#print(len(train))"
      ],
      "metadata": {
        "id": "HbqUebJH77oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플을 확인하기 \n",
        "#train[0][0][:2]\n"
      ],
      "metadata": {
        "id": "epKi2Q2X8mjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training meta (main.py)"
      ],
      "metadata": {
        "id": "XfjJgO9_8tkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "import logging \n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "def random_seed(value):\n",
        "    # 실험 재현성을 위해서 제어해야할 randomness\n",
        "    # PyTorch 작업이 \"결정적\" 알고리즘을 사용해야 하는지 여부를 설정합니다. 즉, 동일한 입력이 주어지고 동일한 소프트웨어 및 하드웨어에서 실행될 때 항상 동일한 출력을 생성하는 알고리즘\n",
        "    torch.backends.cudnn.daterministic=True \n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    np.random.seed(value)\n",
        "    random.seed(value)\n",
        "\n",
        "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
        "    # task 수 만큼 번호 리스트를 만든다.\n",
        "    #print(\"Main create_batch_of_tasks start\")\n",
        "    idxs = list(range(0,len(taskset)))\n",
        "    if is_shuffle:\n",
        "        random.shuffle(idxs)\n",
        "    for i in range(0,len(idxs), batch_size):\n",
        "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size, len(taskset)))] #  yield 여러 개의 데이터를 미리 만들어 놓지 않고 필요할 때마다 즉석해서 하나씩 만들어낼 수 있는 객체를 의미\n",
        "\n",
        "class TrainingArgs:\n",
        "    def __init__(self):\n",
        "        self.num_labels = 3 #2\n",
        "        self.meta_epoch= 10\n",
        "        self.k_spt=20 # 80\n",
        "        self.k_qry=5 #20\n",
        "        self.outer_batch_size = 2\n",
        "        self.inner_batch_size = 12\n",
        "        self.outer_update_lr = 5e-5\n",
        "        self.inner_update_lr = 5e-5\n",
        "        self.inner_update_step = 10\n",
        "        self.inner_update_step_eval = 40\n",
        "        self.bert_model = 'bert-base-uncased'\n",
        "        self.num_task_train = 100 #500\n",
        "        self.num_task_test = 5\n",
        "\n",
        "args = TrainingArgs()\n"
      ],
      "metadata": {
        "id": "O2XE01pv8-uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create meta Learner (Reptile.py) "
      ],
      "metadata": {
        "id": "z3kN-tAfU918"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F \n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler \n",
        "from torch.optim import Adam \n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertForSequenceClassification \n",
        "from copy import deepcopy \n",
        "import gc \n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch \n",
        "import numpy as np\n",
        "\n",
        "class Learner(nn.Module):\n",
        "    \"\"\"\n",
        "    Meta Learner \n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        #print(\"\\nMeta Learner init start\")\n",
        "        \"\"\"\n",
        "        param args:\n",
        "        \"\"\"\n",
        "        super(Learner, self).__init__()\n",
        "        \n",
        "        self.num_labels = args.num_labels\n",
        "        self.outer_batch_size = args.outer_batch_size\n",
        "        self.inner_batch_size = args.inner_batch_size\n",
        "        self.outer_update_lr  = args.outer_update_lr\n",
        "        self.inner_update_lr  = args.inner_update_lr\n",
        "        self.inner_update_step = args.inner_update_step\n",
        "        self.inner_update_step_eval = args.inner_update_step_eval\n",
        "        self.bert_model = args.bert_model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
        "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
        "        self.model.train() #  모델을 학습 모드로 변환\n",
        "\n",
        "    def forward(self, batch_tasks, training = True):\n",
        "        #print(\"Meta Learner forward start \")\n",
        "        torch.cuda.empty_cache()\n",
        "        \"\"\"\n",
        "        batch = [\n",
        "            (support TensorDataset, query TensorDataset),\n",
        "            (support TensorDataset, query TensorDataset),\n",
        "            (support TensorDataset, query TensorDataset),\n",
        "            (support TensorDataset, query TensorDataset),\n",
        "        ]\n",
        "        # support = TensorDataset( all_input_ids, all_attentnion_mask, all_segment_ids, all_label_ids)\n",
        "        \"\"\"\n",
        "        task_accs = []\n",
        "        sum_gradients = []\n",
        "        num_task = len(batch_tasks)\n",
        "        #print(\"batch_tasks : \" , batch_tasks)\n",
        "        # 왜 batch_tasks = 2인 이유 (support 랑 query라서  2)\n",
        "        \n",
        "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
        "        # inner_update_step : 내부 루프의 반복 횟수\n",
        "\n",
        "        #print(\"Meta Learner batch_tasks start\")\n",
        "        f = open('log_label10_loss.txt','a')\n",
        "        for task_id, task in enumerate(batch_tasks):\n",
        "            #print(\"task_id : \", task_id)\n",
        "            support = task[0]\n",
        "            #print(\"Meta Learner batch_tasks len support set : \" , len(support))\n",
        "            query = task[1]\n",
        "            #print(\"Meta Learner batch_tasks len query set : \" , len(query))\n",
        "\n",
        "            # fast_model은 Outer bert를 복제함. 아마도 inner model 인듯? \n",
        "            #print(\"Meta Learner batch_tasks inner model deep copy\")\n",
        "            fast_model = deepcopy(self.model)\n",
        "            fast_model.to(self.device)\n",
        "            # sampler : Dataset을 인자로 받아 data의 index를 반환, shuffle을 하기 위해 사용\n",
        "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
        "                                            batch_size = self.inner_batch_size)\n",
        "            \n",
        "            #print(\"Meta Learner batch_tasks inner model optimizer Adam\")\n",
        "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
        "            fast_model.train()\n",
        "\n",
        "            # innder model loss ?????\n",
        "            \n",
        "            print('----Task', task_id, '---')\n",
        "            f.write('Task : ' + str(task_id) + '\\n')\n",
        "            #print(\"Meta Learner num_inner_update_step : \" , num_inner_update_step)\n",
        "            for i in range(0, num_inner_update_step):\n",
        "                #print(\" i : \", i)\n",
        "                all_loss = [] \n",
        "                for inner_step, batch in enumerate(support_dataloader):\n",
        "                    #print(\"Meta Learner batch 분리 (input_ids, attention_mask, segment_ids, label_id = batch) \")\n",
        "                    batch = tuple(t.to(self.device) for t in batch)\n",
        "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
        "                    #print(\"input_ids : \" , input_ids)\n",
        "                    #print(\"Meta Learner inner_model에 입력 input_ids, attention_mask, segment_ids, label_id\")\n",
        "                    outputs = fast_model(input_ids, attention_mask, segment_ids, labels = label_id)\n",
        "                    #print(\"outputs : \" , outputs)\n",
        "                    loss = outputs[0]\n",
        "                    #print(\"Meta Learner inner_model loss : \", loss)\n",
        "                    loss.backward()\n",
        "                    #print(\"Meta Learner inner_model loss.backward() 실행\")\n",
        "                    inner_optimizer.step()\n",
        "                    #print(\"Meta Learner inner_model inner_optimizer.step() : \", inner_optimizer.step())\n",
        "                    inner_optimizer.zero_grad()\n",
        "                    #print(\"Meta Learner inner_model inner_optimizer.zero_grad 실행\")\n",
        "                    all_loss.append(loss.item())\n",
        "                    #print(\"Meta Learner inner_model all_loss : \" , all_loss)\n",
        "                    \n",
        "                #if i % 4 == 0:\n",
        "                    #print(\"Meta Learner inner_model mean all Loss : \", all_loss) # mean : 배열의 평균 \n",
        "                    #f.write('inner_model mean all Loss : ' + str(np.mean(all_loss)) + '\\n')\n",
        "\n",
        "            fast_model.to(torch.device('cpu'))\n",
        "            \n",
        "            #print(\"Meta Learner inner_model training eval start \")\n",
        "            # outer model과 inner model의 가중치를 비교하여 sum_gradient를 구함 \n",
        "            if training:\n",
        "                #print(\"meta_wights update , len(weights) : \", len(meta_weights))\n",
        "                meta_weights = list(self.model.parameters()) # outer model \n",
        "                fast_weights = list(fast_model.parameters()) # inner model\n",
        "\n",
        "                gradients = []\n",
        "                for i, (meta_params, fast_params) in enumerate(zip(meta_weights, fast_weights)):\n",
        "                    gradient = meta_params - fast_params \n",
        "                    if task_id == 0:\n",
        "                        sum_gradients.append(gradient)\n",
        "                    else:\n",
        "                        sum_gradients[i] += gradient \n",
        "\n",
        "            # query data를 사용해 fast_model 평가 \n",
        "            fast_model.to(self.device)\n",
        "            fast_model.eval() # 학습을 위해 사용하는 Droupout, batchnorm 등을 비활성화 \n",
        "            with torch.no_grad(): # gradient를 계산해주는 context를 비활성화 \n",
        "                query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "                query_batch = iter(query_dataloader).next()\n",
        "                query_batch = tuple(t.to(self.device) for t in query_batch)\n",
        "                q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch \n",
        "                q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
        "\n",
        "                q_logits = F.softmax(q_outputs[1], dim=1)\n",
        "                pre_label_id = torch.argmax(q_logits, dim=1)\n",
        "                pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
        "                q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
        "\n",
        "                acc = accuracy_score(pre_label_id, q_label_id)\n",
        "                #print(\"acc : \", acc)\n",
        "                task_accs.append(acc)\n",
        "\n",
        "            fast_model.to(torch.device('cpu'))\n",
        "            del fast_model, inner_optimizer\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        f.close()\n",
        "\n",
        "        #print(\"Meta Learner outer_model training 2  start\")\n",
        "        if training:\n",
        "            # Average gradient across tasks, 태스크 간 평균 기울기 \n",
        "            for i in range(0, len(sum_gradients)):\n",
        "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
        "            \n",
        "            # 원래 모델에 그라디언트를 할당한 다음 옵티마이저를 사용하여 가중치를 업데이트 함.\n",
        "            for i, params in enumerate(self.model.parameters()):\n",
        "                params.grad = sum_gradients[i]\n",
        "\n",
        "            self.outer_optimizer.step()\n",
        "            self.outer_optimizer.zero_grad()\n",
        "\n",
        "            #print(\"all loss : \" , all_loss)\n",
        "            #print(\"task_accs : \", task_accs)\n",
        "            del sum_gradients\n",
        "            gc.collect()\n",
        "\n",
        "        #print(\"Meta Learner forward end \")\n",
        "        return np.mean(task_accs)"
      ],
      "metadata": {
        "id": "Tu49TDrcU9k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learner = Learner(args)"
      ],
      "metadata": {
        "id": "jlWqBeljiWfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed(123)\n",
        "# low-resource domain example을 태스크 별 support셋과 query 셋으로 분류 ? \n",
        "#test = MetaTask(test_examples, num_task = 3, k_support=50, k_query=20, tokenizer = tokenizer)\n",
        "test = MetaTask(test_examples, num_task = 3, k_support=50, k_query=20, tokenizer = tokenizer)\n",
        "random_seed(int(time.time() % 10))"
      ],
      "metadata": {
        "id": "GnwfP4EYiaJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test.supports[0][1]"
      ],
      "metadata": {
        "id": "GHrCpRFQip72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Start training"
      ],
      "metadata": {
        "id": "kmlutewTglg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0 \n",
        "\n",
        "for epoch in range(args.meta_epoch):\n",
        "    print(\"Start training epoch : \" , epoch)\n",
        "    #train = MetaTask(train_examples, num_task = 500, k_support=80, k_query=20, tokenizer = tokenizer)\n",
        "    train = MetaTask(test_examples, num_task = 10, k_support=20, k_query=5, tokenizer = tokenizer)\n",
        "    #print(\"Start training , train.shape : \" , np.shape(train))\n",
        "    #print(\"Start training len train : \" ,len(train))\n",
        "    db = create_batch_of_tasks(train, is_shuffle = True, batch_size= args.outer_batch_size)\n",
        "    \n",
        "    for step, task_batch in enumerate(db):\n",
        "        \n",
        "        #print(\"task_batch len : \",len(task_batch))\n",
        "        #print(\"for step : \", step)\n",
        "        f = open('log_train3.txt','a')\n",
        "\n",
        "        acc = learner(task_batch)\n",
        "        del task_batch\n",
        "        #print(\"acc : \" , acc)\n",
        "\n",
        "        print('Step:', step, '\\ttraining Acc : ', acc)\n",
        "        f.write('\\n\\nStep : ' + str(step) + '\\n')\n",
        "        f.write('Training Acc : ' + str(acc) + '\\n')\n",
        "        \n",
        "\n",
        "        #if global_step % 20 == 0:\n",
        "        \"\"\"\n",
        "        if global_step % 1 == 0:\n",
        "            random_seed(123)\n",
        "            print(\"\\n--------------------Testing Mode ------------------\\n\")\n",
        "            f.write('Testing Mode \\n')\n",
        "            db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
        "            acc_all_test = []\n",
        "\n",
        "            for test_batch in db_test:\n",
        "                acc = learner(test_batch, training = False)\n",
        "                acc_all_test.append(acc)\n",
        "\n",
        "            print('Step : ', step, 'Test F1 : ', np.mean(acc_all_test))\n",
        "            f.write('Test F1 : ' + str(np.mean(acc_all_test)) + '\\n')\n",
        "\n",
        "            random_seed(int(time.time() % 10))\n",
        "\n",
        "        global_step += 1\n",
        "        \"\"\"\n",
        "        f.close()\n",
        "    del db\n",
        "    #del db_test\n",
        "    #del test_batch\n",
        "\n"
      ],
      "metadata": {
        "id": "2_knYcWthC78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9IttIgqm8NXe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bTwmMzqriNpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ke3jvOSPeLtt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}