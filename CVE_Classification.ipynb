{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMepA9z+Vn6/TnFiO6sjVIw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/Predict-CVE-CWE/blob/main/CVE_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCfFC6KcomhI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/Meta/meta-learning-bert\""
      ],
      "metadata": {
        "id": "6NWzuYViotwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd\n",
        "%ls -al"
      ],
      "metadata": {
        "id": "ot49ketrrVRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/mailong25/meta-learning-bert.git"
      ],
      "metadata": {
        "id": "lEaxneSypFGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Set 확인 "
      ],
      "metadata": {
        "id": "-nCW6CbSx-HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## How to build an accurate sentiment analysis model with handful training examples\n",
        "\n",
        "# 제품 리뷰를 긍정과 부정의 두 가지 범주로 분류하는 모델을 교육하려고 한다.\n",
        "# 우리는 각 제품 및 유형에 대해 단일 모델을 사용한다.\n",
        "# 그러나 일부 도메인에서는 제한된 수의 학습 예제만 가지고 있다. low-resource domain \n",
        "\n",
        "# Let inspect the data\n",
        "import json\n",
        "from random import shuffle\n",
        "# \n",
        "reviews = json.load(open('./dataset.json')) \n",
        "\n",
        "# 데이터 개수 확인  len : 21855\n",
        "# len(reviews)\n",
        "reviews[2000:2005]\n",
        "# 데이터 형태 확인 review text, label : positive or negative , domain : 의류 , 아기 등.\n"
      ],
      "metadata": {
        "id": "Q5zHm_jJpHA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"사무용품\", \"자동차\", \"컴퓨터 & 비디오 게임\"에 대한 100가지 학습 사례가 있다.\n",
        "# 해당 도메인에서도 여전히 정확한 모델을 구축할 수 있을까?  데이터가 매우 적기 때문에 학습이 가능한지 의문을 가지고 있음. \n",
        "\n",
        "# 해결 방안\n",
        "# 많은 학습 데이터를 가진 도메인을 활용하여 좋은 starting point를 만들고 이 시점부터 적은 데이터를 가진 특정 모델에 대한 학습한다. \n",
        "\n",
        "# 접근법 1 \n",
        "# Transfer learning : 많은 학습 데이터를 가진 도메인을 단일 모델에서 학습한다. 그런 다음 적은 데이터를 가진 도메인에서 모델을 재교육한다. \n",
        "\n",
        "# 접근법 2\n",
        "# Meta learning : 적은 학습 데이터만으로 빠르게 학습해야하는 많은 상황을 시뮬레이션한다., 많은 상황을 반복할수록 적은 학습 데이터로 학습하는데 더 좋아지고 있다. \n",
        "# Support set : 적은 훈련 샘플\n",
        "# Query set :  학습 피드백을 제공. 모델은 이 피드백을 사용하여 학습 전략을 조정한다. \n"
      ],
      "metadata": {
        "id": "nz0uWUEdrQbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create  Mata learning Task (task.py)"
      ],
      "metadata": {
        "id": "YMWZCYIKx3Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import torch \n",
        "from torch.utils.data import Dataset \n",
        "import numpy as np \n",
        "import collections \n",
        "import random \n",
        "import json, pickle \n",
        "from torch.utils.data import TensorDataset # DataLoader : 배치를 쉽게 사용, TensorDataset : X, Y 를 텐서로 묶기 위해 사용\n",
        "\n",
        "LABEL_MAP = {'positive' :0 , 'negative' :1, 0:'positive', 1:'negative'}\n",
        "\n",
        "class MetaTask(Dataset):\n",
        "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
        "        \"\"\"\n",
        "        : param sample: list of samples\n",
        "        : param num-task: number of training tasks.\n",
        "        : param k_support : number of support sample per task\n",
        "        : param k_query : number of query sample per task \n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "        random.shuffle(self.examples)\n",
        "\n",
        "        self.num_task = num_task\n",
        "        self.k_support = k_support\n",
        "        self.k_query = k_query\n",
        "        self.tokenizer = tokenizer \n",
        "        self.max_seq_length = 128\n",
        "        self.create_batch(self.num_task)\n",
        "\n",
        "    def create_batch(self, num_task):\n",
        "        self.supports = [] # support set \n",
        "        self.queries = [] # query set\n",
        "\n",
        "        for b in range(num_task): # for each task\n",
        "            # 1. select domain randomly \n",
        "            domain = random.choice(self.examples)['domain'] # 'domain': 'baby'\n",
        "            domainExamples = [e for e in self.examples if e['domain'] == domain] # domainExamples에는 어떤 형태의 데이터가 들어가는가? \n",
        "            \n",
        "            # 1. select k_support + k_query examples from domain randomly \n",
        "            #  random.sample() : 지정한 숫자만큼의 요소들을 랜덤으로 뽑아 리스트로 반환\n",
        "            selected_examples = random.sample(domainExamples, self.k_support + self.k_query) \n",
        "            random.shuffle(selected_examples)\n",
        "            exam_train = selected_examples[:self.k_support]\n",
        "            exam_test = selected_examples[self.k_support:]\n",
        "\n",
        "            self.supports.append(exam_train)\n",
        "            self.queries.append(exam_test)\n",
        "\n",
        "    def create_feature_set(self, examples):\n",
        "        # torch.empty(num1,num2)는 (num1 x num2) 크기의 행렬을 구성하면서 초기화되지 않은 데이터로 값을 채움\n",
        "        all_input_ids       = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_attention_mask   = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_segment_ids     = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_label_ids       = torch.empty(len(examples), dtype = torch.long)\n",
        "\n",
        "        # 변수 뒤에 _ 를 사용하는 이유는 파이썬 기본 시스템 변수명과 충돌을 피하기 위해서이다.\n",
        "        for id_, example in enumerate(examples):\n",
        "            input_ids       = tokenizer.encode(example['text']) # ????? \n",
        "            attention_mask  = [1] * len(input_ids)\n",
        "            segment_ids     = [0] * len(input_ids)\n",
        "\n",
        "            # 패딩\n",
        "            while len(input_ids) < self.max_seq_length: \n",
        "                input_ids.append(0)\n",
        "                attention_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "\n",
        "            label_id = LABEL_MAP[example['label']] # 'label': 'positive' , LABEL_MAP = 'positive' :0 \n",
        "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
        "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
        "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
        "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
        "\n",
        "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
        "        return tensor_set\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        support_set = self.create_feature_set(self.supports[index])\n",
        "        query_set = self.create_feature_set(self.queries[index])\n",
        "        return support_set, query_set \n",
        "\n",
        "    def __len__(self):\n",
        "        # 데이터 세트를 한 묶음으로 만들었기 때문에 집합의 작은 배치 크기를 샘플로 추출할 수 있습니다.\n",
        "        return self.num_task\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "y9YzWGb1x8rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Split meta training and meta testing"
      ],
      "metadata": {
        "id": "dLjHU7ddBlkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low_resource_domains = [\"office_products\", \"automotive\", \"computer_&_video_games\"]\n",
        "train_examples = [ r for r in reviews if r['domain'] not in low_resource_domains]\n",
        "test_examples = [r for r in reviews if r['domain'] in low_resource_domains]\n",
        "print(len(train_examples), len(test_examples))"
      ],
      "metadata": {
        "id": "3rUHtG615j6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "f8-TZPQm6iU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from transformers import BertModel, BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
        "train = MetaTask(train_examples, num_task = 100, k_support=100, k_query=30, tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "31sZvsvt5-TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메타 태스크의 support set 처음 샘플 확인해보기\n",
        "train.supports[0][:2]\n",
        "#train.queries[0][:2]"
      ],
      "metadata": {
        "id": "yMPNhBnT6cFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 메타 태스크의 정보이다. support set과 query set을 포함하는 텐서 데이터세트\n",
        "train[0]\n",
        "#len(train)"
      ],
      "metadata": {
        "id": "HbqUebJH77oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플을 확인하기 \n",
        "train[0][0][:2]\n"
      ],
      "metadata": {
        "id": "epKi2Q2X8mjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training meta (main.py)"
      ],
      "metadata": {
        "id": "XfjJgO9_8tkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "import logging \n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "def random_seed(value):\n",
        "    # 실험 재현성을 위해서 제어해야할 randomness\n",
        "    # PyTorch 작업이 \"결정적\" 알고리즘을 사용해야 하는지 여부를 설정합니다. 즉, 동일한 입력이 주어지고 동일한 소프트웨어 및 하드웨어에서 실행될 때 항상 동일한 출력을 생성하는 알고리즘\n",
        "    torch.backends.cudnn.daterministic=True \n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    np.random.seed(value)\n",
        "    random.seed(value)\n",
        "\n",
        "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
        "    # task 수 만큼 번호 리스트를 만든다.\n",
        "    idxs = list(range(0,len(taskset)))\n",
        "    if is_shuffle:\n",
        "        random.shuffle(idxs)\n",
        "    for i in range(0,len(idxs), batch_size):\n",
        "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size, len(taskset)))] #  yield 여러 개의 데이터를 미리 만들어 놓지 않고 필요할 때마다 즉석해서 하나씩 만들어낼 수 있는 객체를 의미\n",
        "\n",
        "class TrainingArgs:\n",
        "    def __init__(self):\n",
        "        self.num_labels = 2\n",
        "        self.meta_epoch=10\n",
        "        self.k_spt=80\n",
        "        self.k_qry=20\n",
        "        self.outer_batch_size = 2\n",
        "        self.inner_batch_size = 12\n",
        "        self.outer_update_lr = 5e-5\n",
        "        self.inner_update_lr = 5e-5\n",
        "        self.inner_update_step = 10\n",
        "        self.inner_update_step_eval = 40\n",
        "        self.bert_model = 'bert-base-uncased'\n",
        "        self.num_task_train = 500\n",
        "        self.num_task_test = 5\n",
        "\n",
        "args = TrainingArgs()\n"
      ],
      "metadata": {
        "id": "O2XE01pv8-uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create meta Learner (Reptile.py) "
      ],
      "metadata": {
        "id": "z3kN-tAfU918"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F \n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler \n",
        "from torch.optim import Adam \n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertForSequenceClassification \n",
        "from copy import deepcopy \n",
        "import gc \n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch \n",
        "import numpy as np\n",
        "\n",
        "class Learner(nn.Module):\n",
        "    \"\"\"\n",
        "    Meta Learner \n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        param args:\n",
        "        \"\"\"\n",
        "        super(Learner, self).__init__()\n",
        "        \n",
        "        self.num_labels = args.num_labels\n",
        "        self.outer_batch_size = args.outer_batch_size\n",
        "        self.inner_batch_size = args.inner_batch_size\n",
        "        self.outer_update_lr  = args.outer_update_lr\n",
        "        self.inner_update_lr  = args.inner_update_lr\n",
        "        self.inner_update_step = args.inner_update_step\n",
        "        self.inner_update_step_eval = args.inner_update_step_eval\n",
        "        self.bert_model = args.bert_model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
        "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
        "        self.model.train() #  모델을 학습 모드로 변환\n",
        "\n",
        "    def forward(self, batch_tasks, training = True):\n",
        "        \"\"\"\n",
        "        batch = [\n",
        "            (support TensorDataset, query TensorDataset),\n",
        "            (support TensorDataset, query TensorDataset),\n",
        "            (support TensorDataset, query TensorDataset),\n",
        "            (support TensorDataset, query TensorDataset),\n",
        "        ]\n",
        "        # support = TensorDataset( all_input_ids, all_attentnion_mask, all_segment_ids, all_label_ids)\n",
        "        \"\"\"\n",
        "\n",
        "        task_accs = []\n",
        "        sum_gradients = []\n",
        "        num_task = len(batch_tasks)\n",
        "        # 왜 batch_tasks = 2 나왔을까?? support 랑 query라서  2가 나온듯 \n",
        "        \n",
        "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
        "        # inner_update_step : 내부 루프의 반복 횟수\n",
        "\n",
        "        for task_id, task in enumerate(batch_tasks):\n",
        "            print(\"task_id : \", task_id)\n",
        "            support = task[0]\n",
        "            print(\"len support set : \" , len(support))\n",
        "            query = task[1]\n",
        "            print(\"len query set : \" , len(query))\n",
        "\n",
        "            # fast_model은 Outer bert를 복제함. 아마도 inner model 인듯? \n",
        "            fast_model = deepcopy(self.model)\n",
        "            fast_model.to(self.device)\n",
        "            # sampler : Dataset을 인자로 받아 data의 index를 반환, shuffle을 하기 위해 사용\n",
        "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
        "                                            batch_size = self.inner_batch_size)\n",
        "            \n",
        "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
        "            fast_model.train()\n",
        "\n",
        "            # innder model loss ?????\n",
        "            print('----Task', task_id, '---')\n",
        "            print(\"num_inner_update_step : \" , num_inner_update_step)\n",
        "            for i in range(0, num_inner_update_step):\n",
        "                all_loss = [] \n",
        "                for inner_step, batch in enumerate(support_dataloader):\n",
        "\n",
        "                    batch = tuple(t.to(self.device) for t in batch)\n",
        "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
        "                    outputs = fast_model(input_ids, attention_mask, segment_ids, labels = label_id)\n",
        "\n",
        "                    loss = outputs[0]\n",
        "                    loss.backward()\n",
        "                    inner_optimizer.step()\n",
        "                    inner_optimizer.zero_grad()\n",
        "\n",
        "                    all_loss.append(loss.item())\n",
        "                    #print(\"all_loss : \" , all_loss)\n",
        "\n",
        "                if i % 4 == 0:\n",
        "                    print(\"Inner Loss : \", np.mean(all_loss)) # mean : 배열의 평균 \n",
        "\n",
        "            fast_model.to(torch.device('cpu'))\n",
        "\n",
        "            # outer model과 inner model의 가중치를 비교하여 sum_gradient를 구함 \n",
        "            if training:\n",
        "                print(\"meta_wights update , len(weights) : \", len(meta_weights))\n",
        "                meta_weights = list(self.model.parameters()) # outer model \n",
        "                fast_weights = list(fast_model.parameters()) # inner model\n",
        "\n",
        "                gradients = []\n",
        "                for i, (meta_params, fast_params) in enumerate(zip(meta_weights, fast_weights)):\n",
        "                    gradient = meta_params - fast_params \n",
        "                    if task_id == 0:\n",
        "                        sum_gradients.append(gradient)\n",
        "                    else:\n",
        "                        sum_gradients[i] += gradient \n",
        "\n",
        "            # query data를 사용해 fast_model 평가 \n",
        "            fast_model.to(self.device)\n",
        "            fast_model.eval() # 학습을 위해 사용하는 Droupout, batchnorm 등을 비활성화 \n",
        "            with torch.no_grad(): # gradient를 계산해주는 context를 비활성화 \n",
        "                query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "                query_batch = iter(query_dataloader).next()\n",
        "                query_batch = tuple(t.to(self.device) for t in query_batch)\n",
        "                q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch \n",
        "                q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
        "\n",
        "                q_logits = F.softmax(q_outputs[1], dim=1)\n",
        "                pre_label_id = torch.argmax(q_logits, dim=1)\n",
        "                pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
        "                q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
        "\n",
        "                acc = accuracy_score(pre_label_id, q_label_id)\n",
        "                task_accs.append(acc)\n",
        "\n",
        "            fast_model.to(torch.device('cpu'))\n",
        "            del fast_model, inner_optimizer\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if training:\n",
        "            # Average gradient across tasks, 태스크 간 평균 기울기 \n",
        "            for i in range(0, len(sum_gradients)):\n",
        "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
        "            \n",
        "            # 원래 모델에 그라디언트를 할당한 다음 옵티마이저를 사용하여 가중치를 업데이트 함.\n",
        "            for i, params in enumerate(self.model.parameters()):\n",
        "                params.grad = sum_gradients[i]\n",
        "\n",
        "            self.outer_optimizer.step()\n",
        "            self.outer_optimizer.zero_grad()\n",
        "\n",
        "            del sum_gradients\n",
        "            gc.collect()\n",
        "\n",
        "        return np.mean(task_accs)"
      ],
      "metadata": {
        "id": "Tu49TDrcU9k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learner = Learner(args)"
      ],
      "metadata": {
        "id": "jlWqBeljiWfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed(123)\n",
        "# low-resource domain example을 태스크 별 support셋과 query 셋으로 분류 ? \n",
        "test = MetaTask(test_examples, num_task = 3, k_support=80, k_query=20, tokenizer = tokenizer)\n",
        "random_seed(int(time.time() % 10))"
      ],
      "metadata": {
        "id": "GnwfP4EYiaJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test.supports[2]"
      ],
      "metadata": {
        "id": "GHrCpRFQip72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Start training"
      ],
      "metadata": {
        "id": "kmlutewTglg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0 \n",
        "\n",
        "for epoch in range(args.meta_epoch):\n",
        "\n",
        "    #train = MetaTask(train_examples, num_task = 500, k_support=80, k_query=20, tokenizer = tokenizer)\n",
        "    train = MetaTask(train_examples, num_task = 100, k_support=80, k_query=20, tokenizer = tokenizer)\n",
        "    db = create_batch_of_tasks(train, is_shuffle = True, batch_size= args.outer_batch_size)\n",
        "    \n",
        "    for step, task_batch in enumerate(db):\n",
        "        #print(\"task_batch len : \",len(task_batch))\n",
        "        #print(\"for step : \", step)\n",
        "        f = open('log.txt','a')\n",
        "\n",
        "        acc = learner(task_batch)\n",
        "        # print(\"acc : \" , acc)\n",
        "\n",
        "        print('Step:', step, '\\ttraining Acc : ', acc)\n",
        "        f.write(str(acc) + '\\n')\n",
        "\n",
        "        if global_step % 20 == 0:\n",
        "            random_seed(123)\n",
        "            print(\"\\n--------------------Testing Mode ------------------\\n\")\n",
        "            db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
        "            acc_all_test = []\n",
        "\n",
        "            for test_batch in db_test:\n",
        "                acc = learner(test_batch, training = False)\n",
        "                acc_all_test.append(acc)\n",
        "\n",
        "            print('Step : ', step, 'Test F1 : ', np.mean(acc_all_test))\n",
        "            f.write('Test ' + str(np.mean(acc_all_test)) + '\\n')\n",
        "\n",
        "            random_seed(int(time.time() % 10))\n",
        "\n",
        "        global_step += 1\n",
        "        f.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "2_knYcWthC78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bTwmMzqriNpC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}